{
  "Metadata": {
    "inverse": {
      "title": "Inverse Matrix Calculator & Solver with Steps (2x2, 3x3, 4x4) - Free",
      "description": "Calculate the inverse matrix instantly with detailed step-by-step explanations. Supports Adjoint and Gauss-Jordan methods. Perfect for linear algebra homework.",
      "h1": "Inverse Matrix Calculator & Solver"
    },
    "rref": {
      "title": "RREF Calculator & Solver with Steps - Reduced Row Echelon Form",
      "description": "Solve systems of linear equations using this RREF calculator. Shows every row operation step-by-step to help you learn.",
      "h1": "Reduced Row Echelon Form (RREF) Calculator & Solver"
    },
    "multiplication": {
      "title": "Matrix Multiplication Calculator & Solver (Dot Product)",
      "description": "Multiply matrices instantly with this free calculator. Supports 2x2, 3x3, and larger matrices. Perfect for engineering and data science.",
      "h1": "Matrix Multiplication Calculator & Solver"
    },
    "determinant": {
      "title": "Matrix Determinant Calculator & Solver with Steps",
      "description": "Calculate the determinant of any matrix with step-by-step explanations. Supports 2x2, 3x3, 4x4 matrices and beyond.",
      "h1": "Matrix Determinant Calculator & Solver"
    },
    "eigenvalue": {
      "title": "Eigenvalue Calculator & Solver with Steps",
      "description": "Find eigenvalues and eigenvectors of matrices with detailed step-by-step solutions. Perfect for linear algebra students.",
      "h1": "Eigenvalue Calculator & Solver"
    },
    "rank": {
      "title": "Matrix Rank Calculator & Solver",
      "description": "Calculate the rank of a matrix instantly. Shows the row reduction steps to find the rank.",
      "h1": "Matrix Rank Calculator & Solver"
    },
    "system-equations": {
      "title": "System of Linear Equations Solver with Steps - Free Calculator",
      "description": "Solve systems of linear equations using Gaussian elimination. Enter coefficients as an augmented matrix and get step-by-step solutions with exact fractions.",
      "h1": "System of Linear Equations Solver"
    },
    "cramers-rule": {
      "title": "Cramer's Rule Calculator with Steps - Free Solver",
      "description": "Solve systems of linear equations using Cramer's rule. Calculate determinants step-by-step to find solutions. Perfect for 2x2 and 3x3 systems.",
      "h1": "Cramer's Rule Calculator"
    },
    "matrix-power": {
      "title": "Matrix Power Calculator (A^n) with Steps - Free",
      "description": "Calculate matrix powers A^2, A^3, A^n with step-by-step solutions. Shows each multiplication step. Supports any square matrix.",
      "h1": "Matrix Power Calculator"
    }
  },
  "Common": {
    "calculate": "Calculate",
    "clear": "Clear",
    "showSteps": "Show Solution Steps",
    "hideSteps": "Hide Solution Steps",
    "copyLatex": "Copy LaTeX",
    "exportImage": "Export Image",
    "result": "Result",
    "steps": "Solution Steps",
    "matrixSize": "Matrix Size",
    "rows": "Rows",
    "columns": "Columns",
    "relatedTools": "You Might Also Need"
  },
  "Footer": {
    "aboutTitle": "About",
    "aboutText": "Free matrix calculators with step-by-step solutions for linear algebra students and professionals.",
    "quickLinksTitle": "Quick Links",
    "home": "Home",
    "about": "About Us",
    "legalTitle": "Legal",
    "privacy": "Privacy Policy",
    "terms": "Terms of Service",
    "copyright": "Matrix Calculator. All rights reserved."
  },
  "Privacy": {
    "title": "Privacy Policy",
    "description": "Learn how we protect your privacy and handle your data.",
    "section1Title": "Data Collection",
    "section1Content": "We do not collect or store any personal data or matrix calculations. All computations are performed locally in your browser or on our server without logging any input data.",
    "section2Title": "Cookies",
    "section2Content": "This website uses minimal cookies only for essential functionality. We do not use tracking cookies or third-party analytics.",
    "section3Title": "Your Privacy Rights",
    "section3Content": "Since we don't collect any personal data, there is no data to request, modify, or delete. Your calculations remain completely private.",
    "section4Title": "Third-Party Services",
    "section4Content": "We may use third-party services for hosting and CDN delivery. These services do not have access to your calculation data.",
    "section5Title": "Updates to Privacy Policy",
    "section5Content": "We may update this privacy policy from time to time. Any changes will be posted on this page with an updated revision date."
  },
  "Terms": {
    "title": "Terms of Service",
    "description": "Terms and conditions for using our matrix calculators.",
    "section1Title": "Acceptance of Terms",
    "section1Content": "By using this website, you agree to these terms of service. If you do not agree, please do not use the calculators.",
    "section2Title": "Use License",
    "section2Content": "You are granted a limited license to use these calculators for personal, educational, and commercial purposes. You may not copy, modify, or distribute the source code without permission.",
    "section3Title": "Accuracy Disclaimer",
    "section3Content": "While we strive for accuracy, we do not guarantee that all calculations are error-free. Always verify critical calculations independently.",
    "section4Title": "Limitations",
    "section4Content": "We are not liable for any damages arising from the use or inability to use these calculators. Use at your own risk."
  },
  "About": {
    "title": "About Us",
    "description": "Learn about our mission to make linear algebra accessible to everyone.",
    "missionTitle": "Our Mission",
    "missionContent": "We created these free matrix calculators to help students, engineers, and data scientists solve linear algebra problems quickly and understand the underlying mathematics through step-by-step explanations.",
    "storyTitle": "Our Story",
    "storyContent": "As former math students, we remember struggling with tedious matrix calculations and wishing for a tool that not only gave answers but also showed the complete solution process. That's why we built this platform—to make linear algebra more accessible and less intimidating.",
    "whyFreeTitle": "Why Free?",
    "whyFreeContent": "Education should be accessible to everyone. We believe in providing high-quality mathematical tools without paywalls or registration requirements. All our calculators are free to use, forever.",
    "technologyTitle": "Technology",
    "technologyContent": "Our calculators use precise fraction-based arithmetic to ensure accuracy. All computations can be performed client-side for maximum privacy, with step-by-step explanations generated using advanced algorithms.",
    "contactTitle": "Get in Touch",
    "contactContent": "Have questions or suggestions? We'd love to hear from you. Use the contact information in the footer to reach out."
  },
  "Inverse": {
    "step1": "Step 1: Find the Determinant",
    "step1Desc": "First, we calculate the determinant of the matrix. If it is 0, the inverse does not exist.",
    "step2": "Step 2: Find the Cofactor Matrix",
    "step2Desc": "Calculate the cofactor for each element.",
    "step3": "Step 3: Find the Adjugate Matrix",
    "step3Desc": "Transpose the cofactor matrix.",
    "step4": "Step 4: Multiply by 1/Determinant",
    "step4Desc": "Multiply the adjugate matrix by 1 over the determinant.",
    "noInverse": "This matrix has no inverse (determinant = 0)."
  },
  "RREF": {
    "step1": "Step 1: Create Augmented Matrix",
    "step1Desc": "Write the system as an augmented matrix.",
    "step2": "Step 2: Row Operations",
    "step2Desc": "Apply Gaussian elimination to get row echelon form.",
    "step3": "Step 3: Back Substitution",
    "step3Desc": "Apply back substitution to get reduced row echelon form."
  },
  "FAQ": {
    "inverse": {
      "q1": "How to find the inverse of a 3x3 matrix?",
      "a1": "To find the inverse of a 3x3 matrix, follow these steps: 1) Calculate the determinant - if it's zero, the matrix is singular and has no inverse. 2) Find the matrix of cofactors by calculating the determinant of each 2x2 minor matrix. 3) Transpose the cofactor matrix to get the adjugate matrix. 4) Multiply the adjugate matrix by 1/determinant to get the inverse.",
      "q2": "When does a matrix have no inverse?",
      "a2": "A matrix has no inverse if its determinant is zero. Such matrices are called singular or non-invertible matrices. This occurs when the rows or columns are linearly dependent, meaning one row is a linear combination of the others.",
      "q3": "What is the adjugate matrix?",
      "a3": "The adjugate matrix (also called the adjoint matrix) is the transpose of the cofactor matrix. It's a key component in the formula for finding the inverse: A⁻¹ = (1/det(A)) × adj(A). Each element in the adjugate is a signed minor determinant.",
      "q4": "What are the applications of inverse matrices?",
      "a4": "Inverse matrices are used extensively in: 1) Solving systems of linear equations (Ax = b becomes x = A⁻¹b), 2) Computer graphics for transformations and camera positioning, 3) Cryptography for encryption and decryption, 4) Control theory for system analysis, 5) Statistics for regression analysis."
    },
    "rref": {
      "q1": "What is reduced row echelon form?",
      "a1": "Reduced row echelon form (RREF) is a special form of a matrix with these properties: 1) All zero rows are at the bottom, 2) The leading entry (pivot) in each non-zero row is 1, 3) Each leading 1 is the only non-zero entry in its column, 4) Each leading 1 appears to the right of the leading 1 in the row above.",
      "q2": "How to convert a matrix to RREF?",
      "a2": "Use Gaussian-Jordan elimination: 1) Start with row echelon form using forward elimination, 2) Scale each pivot row so the leading entry becomes 1, 3) Eliminate all entries above each pivot by adding multiples of the pivot row to rows above it, 4) Continue until all conditions for RREF are satisfied.",
      "q3": "What is the difference between REF and RREF?",
      "a3": "Row Echelon Form (REF) requires only zeros below each pivot, while Reduced Row Echelon Form (RREF) requires zeros both above and below each pivot. RREF also requires that all pivots equal 1. RREF is unique for any matrix, while REF is not.",
      "q4": "Why use RREF for solving systems?",
      "a4": "RREF makes solving systems of linear equations trivial because the solution can be read directly from the final matrix. Each pivot column corresponds to a basic variable, and the rightmost column (in an augmented matrix) gives the values. It also clearly reveals whether a system has no solution, one solution, or infinitely many solutions."
    }
  },
  "Article": {
    "inverse": {
      "title": "Understanding Matrix Inverses: A Complete Guide",
      "introduction": "The inverse of a matrix is one of the most fundamental concepts in linear algebra. If you've ever solved a system of equations or worked with transformations, you've likely encountered the need to find a matrix inverse. This comprehensive guide will teach you not just how to use our calculator, but how to manually compute matrix inverses and understand their real-world applications.\n\nWhether you're a student learning linear algebra for the first time or a professional working with computational methods, understanding matrix inverses is essential. The inverse matrix allows us to \"undo\" matrix operations, solve systems of equations efficiently, and perform complex transformations in computer graphics and engineering.",
      "definitionTitle": "What is an Inverse Matrix?",
      "definition": "For a square matrix A, the inverse matrix (denoted as A⁻¹) is the unique matrix such that A × A⁻¹ = A⁻¹ × A = I, where I is the identity matrix. Not all matrices have inverses—only square matrices with a non-zero [determinant] are invertible. Matrices with zero determinants are called singular or non-invertible matrices.\n\nThe existence of an inverse depends on the matrix's [determinant]. If det(A) = 0, the matrix is singular and has no inverse. This occurs when the rows or columns of the matrix are linearly dependent, meaning one row can be expressed as a combination of the others.",
      "calculationTitle": "How to Calculate the Inverse Matrix Manually",
      "calculation": "There are two primary methods for finding the inverse of a matrix: the Adjoint Method and the Gauss-Jordan Elimination Method.\n\n**Method 1: Adjoint Method (for 2×2 and 3×3 matrices)**\n\nFor a 2×2 matrix, the inverse can be found using a simple formula. For larger matrices, we use the cofactor expansion method:\n\n1. **Calculate the Determinant**: First, find the [determinant] of the matrix. If it's zero, stop—the matrix has no inverse.\n\n2. **Find the Cofactor Matrix**: For each element aᵢⱼ, calculate its cofactor Cᵢⱼ = (-1)ⁱ⁺ʲ × Mᵢⱼ, where Mᵢⱼ is the minor (determinant of the submatrix formed by removing row i and column j).\n\n3. **Form the Adjugate Matrix**: Transpose the cofactor matrix to get the adjugate (also called adjoint) matrix.\n\n4. **Multiply by 1/Determinant**: Multiply each element of the adjugate matrix by 1/det(A) to get the inverse.\n\nThe formula is: A⁻¹ = (1/det(A)) × adj(A)\n\n**Method 2: Gauss-Jordan Elimination**\n\nThis method is more systematic and works well for larger matrices:\n\n1. Form the augmented matrix [A | I], where I is the identity matrix of the same size.\n2. Apply row operations to transform A into the identity matrix I.\n3. The right side of the augmented matrix will become A⁻¹.\n\nThis method is particularly useful when combined with [rref] techniques, as it systematically reduces the matrix to its simplest form.",
      "applicationsTitle": "Real-World Applications of Inverse Matrices",
      "applications": "Inverse matrices are ubiquitous in mathematics, engineering, and computer science:\n\n**1. Solving Systems of Linear Equations**: The most common application. Given Ax = b, we can find x = A⁻¹b. This is computationally efficient for systems with many equations.\n\n**2. Computer Graphics**: Inverse matrices are essential for 3D transformations. When you rotate or scale an object, the inverse transformation allows you to reverse the operation. Camera positioning in 3D games and animation relies heavily on matrix inverses.\n\n**3. Cryptography**: Many encryption algorithms use matrix operations. The inverse matrix is used for decryption, allowing secure communication.\n\n**4. Control Theory**: In engineering, inverse matrices help analyze system stability and design controllers for robots, aircraft, and industrial processes.\n\n**5. Statistics and Machine Learning**: Regression analysis, principal component analysis (PCA), and many machine learning algorithms use matrix inverses to find optimal solutions.\n\n**6. Economics**: Input-output models in economics use inverse matrices to analyze how changes in one sector affect others in an economy.",
      "formulaTitle": "Key Formula",
      "formula": "For a 2×2 matrix A = [a b; c d], the inverse is: A⁻¹ = (1/(ad-bc)) × [d -b; -c a]"
    },
    "rref": {
      "title": "Reduced Row Echelon Form (RREF): Complete Tutorial",
      "introduction": "Reduced Row Echelon Form, or RREF, is the \"gold standard\" form of a matrix in linear algebra. It's the simplest possible form a matrix can take while preserving all the information about a system of linear equations. Mastering RREF is essential for solving systems of equations, finding matrix [rank], and understanding linear transformations.\n\nThis guide will walk you through what RREF is, why it matters, and how to manually convert any matrix to RREF using Gaussian-Jordan elimination. By the end, you'll understand not just how to use our calculator, but the mathematical principles behind it.",
      "definitionTitle": "What is Reduced Row Echelon Form?",
      "definition": "A matrix is in Reduced Row Echelon Form (RREF) when it satisfies these four conditions:\n\n1. **All zero rows are at the bottom**: Any row containing only zeros must be below all non-zero rows.\n\n2. **Leading entry is 1**: The first non-zero entry (called the pivot or leading entry) in each non-zero row must be 1.\n\n3. **Pivots are the only non-zero in their column**: Each column containing a pivot must have zeros everywhere else.\n\n4. **Pivots move to the right**: Each pivot must be to the right of the pivot in the row above it.\n\nRREF is unique—every matrix has exactly one RREF form, regardless of how you get there. This makes it incredibly useful for solving systems of equations, as the solution can be read directly from the RREF matrix.",
      "calculationTitle": "How to Convert a Matrix to RREF Manually",
      "calculation": "The process of converting a matrix to RREF is called Gaussian-Jordan elimination. Here's the step-by-step method:\n\n**Step 1: Forward Elimination (Get to Row Echelon Form)**\n\n1. Start with the leftmost non-zero column. This will be your first pivot column.\n2. If the top entry is zero, swap rows to bring a non-zero entry to the top.\n3. Use row operations to make all entries below the pivot zero. Add multiples of the pivot row to rows below it.\n4. Move to the next column and repeat, working from top to bottom and left to right.\n\n**Step 2: Scale Pivots to 1**\n\n1. For each pivot row, divide the entire row by the pivot value to make it equal to 1.\n2. This ensures all pivots are 1, as required by RREF.\n\n**Step 3: Backward Elimination (Get to RREF)**\n\n1. Start with the rightmost pivot and work upward.\n2. For each pivot, use row operations to eliminate all entries above it.\n3. Add multiples of the pivot row to rows above it to create zeros.\n4. Continue until all entries above each pivot are zero.\n\n**Example Row Operations:**\n- Swap two rows: Rᵢ ↔ Rⱼ\n- Multiply a row by a scalar: Rᵢ → kRᵢ\n- Add a multiple of one row to another: Rᵢ → Rᵢ + kRⱼ\n\nThis process is closely related to finding the [inverse] of a matrix, as both use similar elimination techniques.",
      "applicationsTitle": "Applications of RREF",
      "applications": "RREF is the foundation for solving many linear algebra problems:\n\n**1. Solving Systems of Linear Equations**: RREF makes solving systems trivial. Each pivot column corresponds to a basic variable, and you can read the solution directly from the augmented matrix. If you get a row like [0 0 0 | 5], the system has no solution. If you have free variables, the system has infinitely many solutions.\n\n**2. Finding Matrix Rank**: The [rank] of a matrix equals the number of pivots in its RREF. This tells you the dimension of the column space and row space.\n\n**3. Finding the Inverse Matrix**: To find A⁻¹, form [A | I] and convert to RREF. If you get [I | B], then B = A⁻¹. If you can't get I on the left, the matrix has no [inverse].\n\n**4. Determining Linear Independence**: Vectors are linearly independent if and only if the RREF of the matrix formed by these vectors has no zero rows.\n\n**5. Finding Bases**: The pivot columns of the original matrix form a basis for the column space. The non-zero rows of RREF form a basis for the row space.\n\n**6. Computer Science**: RREF algorithms are used in computer graphics, optimization problems, and solving constraint systems in game engines and CAD software."
    },
    "determinant": {
      "title": "Matrix Determinant: Theory and Calculation Methods",
      "introduction": "The determinant is a single number that captures essential information about a square matrix. It tells us whether a matrix is invertible, the volume scaling factor of linear transformations, and much more. Understanding determinants is crucial for working with [inverse] matrices, solving systems of equations, and analyzing linear transformations.\n\nThis comprehensive guide covers everything you need to know about determinants: what they are, how to calculate them manually, and their numerous applications in mathematics and engineering.",
      "definitionTitle": "What is a Determinant?",
      "definition": "The determinant of a square matrix A (denoted as det(A) or |A|) is a scalar value that encodes important properties of the matrix. For a 2×2 matrix, the determinant is calculated as ad - bc. For larger matrices, we use expansion by minors or row reduction methods.\n\nThe determinant has several key properties:\n\n- **Invertibility Test**: A matrix has an [inverse] if and only if its determinant is non-zero. If det(A) = 0, the matrix is singular (non-invertible).\n- **Volume Scaling**: The absolute value of the determinant represents how much a linear transformation scales volumes in n-dimensional space.\n- **Orientation**: The sign of the determinant indicates whether the transformation preserves (positive) or reverses (negative) orientation.\n- **Product Rule**: det(AB) = det(A) × det(B) for any two square matrices of the same size.",
      "calculationTitle": "How to Calculate Determinants Manually",
      "calculation": "There are several methods for calculating determinants, each suited to different matrix sizes:\n\n**Method 1: Formula for 2×2 Matrices**\n\nFor A = [a b; c d], the determinant is simply: det(A) = ad - bc\n\n**Method 2: Cofactor Expansion (Laplace Expansion)**\n\nFor larger matrices, expand along any row or column:\n\n1. Choose a row or column (preferably one with many zeros).\n2. For each element aᵢⱼ in that row/column, calculate its cofactor: Cᵢⱼ = (-1)ⁱ⁺ʲ × Mᵢⱼ\n3. The determinant is the sum: det(A) = Σ aᵢⱼ × Cᵢⱼ\n\nWhere Mᵢⱼ is the minor (determinant of the submatrix after removing row i and column j).\n\n**Method 3: Row Reduction**\n\n1. Convert the matrix to upper triangular form using row operations.\n2. The determinant is the product of the diagonal entries, multiplied by (-1)ⁿ where n is the number of row swaps performed.\n\n**Important**: When using row operations:\n- Swapping rows multiplies the determinant by -1\n- Multiplying a row by k multiplies the determinant by k\n- Adding a multiple of one row to another doesn't change the determinant",
      "applicationsTitle": "Applications of Determinants",
      "applications": "Determinants appear throughout mathematics and its applications:\n\n**1. Matrix Invertibility**: The most immediate application. A matrix has an [inverse] if det(A) ≠ 0. This is why calculating the [determinant] is the first step when finding the [inverse] of a matrix.\n\n**2. Solving Systems of Equations (Cramer's Rule)**: For a system Ax = b, if det(A) ≠ 0, each variable xᵢ can be found by replacing column i of A with b and dividing by det(A).\n\n**3. Area and Volume Calculations**: In 2D, the determinant of a 2×2 matrix gives the area of the parallelogram spanned by its column vectors. In 3D, the determinant of a 3×3 matrix gives the volume of the parallelepiped.\n\n**4. Linear Transformations**: The determinant tells you how much a linear transformation scales areas/volumes. A determinant of 2 means areas are doubled; -1 means areas are preserved but orientation is flipped.\n\n**5. Eigenvalues**: The determinant is used in finding [eigenvalue]s, as det(A - λI) = 0 is the characteristic equation.\n\n**6. Cross Products**: In 3D vector calculus, the cross product can be computed using a determinant formula.\n\n**7. Change of Variables in Integration**: The Jacobian determinant appears when changing variables in multiple integrals."
    },
    "multiplication": {
      "title": "Matrix Multiplication: Complete Guide with Examples",
      "introduction": "Matrix multiplication is one of the most fundamental operations in linear algebra. Unlike regular multiplication, matrix multiplication has specific rules and properties that make it powerful for representing linear transformations, solving systems of equations, and performing computations in computer graphics and machine learning.\n\nThis guide will teach you how to multiply matrices by hand, understand when multiplication is possible, and explore the many applications of this operation. You'll also learn how matrix multiplication relates to finding [inverse] matrices and computing [determinant]s.",
      "definitionTitle": "What is Matrix Multiplication?",
      "definition": "Matrix multiplication is an operation that combines two matrices to produce a third matrix. For two matrices A (m×n) and B (n×p) to be multipliable, the number of columns in A must equal the number of rows in B. The result is a matrix C (m×p).\n\nThe element cᵢⱼ in the product matrix is calculated as the dot product of row i of A and column j of B:\n\ncᵢⱼ = Σₖ aᵢₖ × bₖⱼ\n\n**Key Properties:**\n- Matrix multiplication is NOT commutative: AB ≠ BA in general\n- Matrix multiplication IS associative: (AB)C = A(BC)\n- Matrix multiplication distributes over addition: A(B + C) = AB + AC\n- The identity matrix I acts as a multiplicative identity: AI = IA = A",
      "calculationTitle": "How to Multiply Matrices Manually",
      "calculation": "Here's the step-by-step process for multiplying two matrices:\n\n**Step 1: Verify Compatibility**\n\nCheck that the number of columns in the first matrix equals the number of rows in the second matrix. For A (m×n) × B (n×p), this condition must be satisfied.\n\n**Step 2: Set Up the Result Matrix**\n\nThe product will be an m×p matrix. Create a grid of this size.\n\n**Step 3: Calculate Each Element**\n\nFor each position (i, j) in the result matrix:\n1. Take row i from the first matrix\n2. Take column j from the second matrix\n3. Multiply corresponding elements and sum them up\n\n**Example:**\n\nFor A = [1 2; 3 4] and B = [5 6; 7 8]:\n\n- c₁₁ = (1×5) + (2×7) = 5 + 14 = 19\n- c₁₂ = (1×6) + (2×8) = 6 + 16 = 22\n- c₂₁ = (3×5) + (4×7) = 15 + 28 = 43\n- c₂₂ = (3×6) + (4×8) = 18 + 32 = 50\n\nResult: [19 22; 43 50]\n\n**Visual Method:**\n\n1. Place the first matrix on the left\n2. Place the second matrix on the top\n3. Each cell in the result is the intersection of a row and column",
      "applicationsTitle": "Applications of Matrix Multiplication",
      "applications": "Matrix multiplication is the foundation of many computational techniques:\n\n**1. Linear Transformations**: Matrix multiplication represents linear transformations. Rotating, scaling, or shearing objects in computer graphics all use matrix multiplication. Composing transformations is done by multiplying their matrices.\n\n**2. Solving Systems of Equations**: Systems Ax = b can be solved using matrix operations. If A has an [inverse], we compute x = A⁻¹b, which involves matrix [multiplication].\n\n**3. Computer Graphics**: Every 3D transformation—rotation, translation, scaling, projection—is represented as matrix multiplication. Game engines and animation software rely heavily on this.\n\n**4. Machine Learning**: Neural networks are essentially chains of matrix multiplications. Each layer multiplies input data by weight matrices, applying activation functions between layers.\n\n**5. Network Analysis**: Adjacency matrices in graph theory are multiplied to find paths of certain lengths between nodes. Powers of adjacency matrices reveal connectivity patterns.\n\n**6. Cryptography**: Many encryption algorithms use matrix multiplication. The security relies on the difficulty of finding the [inverse] of certain matrices.\n\n**7. Economics**: Input-output models use matrix multiplication to analyze how production in one sector affects others. Leontief models are built entirely on matrix operations.\n\n**8. Quantum Mechanics**: Quantum states and operators are represented as matrices, and their evolution involves matrix multiplication."
    },
    "eigenvalue": {
      "title": "Eigenvalues and Eigenvectors: A Comprehensive Guide",
      "introduction": "Eigenvalues and eigenvectors are among the most important concepts in linear algebra, with applications spanning from quantum mechanics to Google's PageRank algorithm. They reveal the fundamental \"directions\" and \"scalings\" of linear transformations, making them essential for understanding how matrices behave.\n\nThis guide will teach you what eigenvalues and eigenvectors are, how to compute them manually, and why they're so crucial in mathematics, physics, and computer science. Understanding eigenvalues also helps when working with [determinant]s and [inverse] matrices.",
      "definitionTitle": "What are Eigenvalues and Eigenvectors?",
      "definition": "For a square matrix A, an eigenvector v is a non-zero vector that, when multiplied by A, results in a scalar multiple of itself:\n\nAv = λv\n\nHere, λ (lambda) is called the eigenvalue corresponding to the eigenvector v. In essence, eigenvectors are directions that remain unchanged (except for scaling) when the linear transformation represented by A is applied.\n\n**Key Properties:**\n- Eigenvalues can be real or complex numbers\n- An n×n matrix has at most n distinct eigenvalues (counting multiplicity)\n- Eigenvectors corresponding to distinct eigenvalues are linearly independent\n- The sum of eigenvalues equals the trace of the matrix\n- The product of eigenvalues equals the [determinant] of the matrix\n- If 0 is an eigenvalue, the matrix is singular (has no [inverse])",
      "calculationTitle": "How to Find Eigenvalues and Eigenvectors Manually",
      "calculation": "**Finding Eigenvalues:**\n\n1. Start with the characteristic equation: det(A - λI) = 0, where I is the identity matrix.\n2. Compute the [determinant] of (A - λI). This gives you a polynomial in λ called the characteristic polynomial.\n3. Solve the characteristic polynomial to find the roots. These roots are the eigenvalues.\n\n**Example for 2×2 Matrix:**\n\nFor A = [a b; c d], we solve:\n\ndet([a-λ b; c d-λ]) = (a-λ)(d-λ) - bc = 0\n\nThis expands to: λ² - (a+d)λ + (ad-bc) = 0\n\n**Finding Eigenvectors:**\n\nFor each eigenvalue λᵢ:\n\n1. Solve the system (A - λᵢI)v = 0\n2. This is equivalent to finding the null space of (A - λᵢI)\n3. Use [rref] techniques to solve the homogeneous system\n4. The non-zero solutions are the eigenvectors\n\n**Step-by-Step Process:**\n\n1. For each eigenvalue, form the matrix (A - λI)\n2. Use Gaussian elimination or [rref] to solve (A - λI)v = 0\n3. Express the solution in parametric form\n4. The free variables give you the eigenvector directions",
      "applicationsTitle": "Applications of Eigenvalues and Eigenvectors",
      "applications": "Eigenvalues and eigenvectors have countless applications:\n\n**1. Principal Component Analysis (PCA)**: In data science, PCA uses eigenvectors to find the directions of maximum variance in high-dimensional data. The eigenvalues indicate the importance of each principal component.\n\n**2. Google PageRank**: The PageRank algorithm finds the dominant eigenvector of the web's link matrix to rank web pages. The largest eigenvalue corresponds to the steady-state distribution of web surfers.\n\n**3. Quantum Mechanics**: In quantum mechanics, observables (like energy, momentum) are represented by operators (matrices). The eigenvalues are the possible measurement values, and eigenvectors are the corresponding quantum states.\n\n**4. Vibration Analysis**: In engineering, eigenvalues represent natural frequencies of vibrating systems. Engineers use this to design structures that avoid resonance.\n\n**5. Stability Analysis**: In control theory and differential equations, eigenvalues determine system stability. Negative real parts mean stable systems; positive mean unstable.\n\n**6. Image Processing**: Eigenfaces in facial recognition use eigenvectors to compress and recognize faces. The technique finds the most important \"directions\" in face space.\n\n**7. Markov Chains**: The steady-state distribution of a Markov chain is given by the eigenvector corresponding to eigenvalue 1.\n\n**8. Graph Theory**: The eigenvalues of adjacency matrices reveal properties of graphs, such as connectivity and expansion properties."
    },
    "rank": {
      "title": "Matrix Rank: Understanding Linear Independence and Dimension",
      "introduction": "The rank of a matrix is a fundamental concept that tells us about the linear independence of its rows and columns, the dimension of its column and row spaces, and whether systems of equations have solutions. Understanding rank is essential for working with [rref], finding [inverse] matrices, and solving systems of linear equations.\n\nThis guide explains what matrix rank means, how to compute it manually using row reduction, and its many applications in linear algebra and beyond.",
      "definitionTitle": "What is Matrix Rank?",
      "definition": "The rank of a matrix A (denoted as rank(A) or rk(A)) is the maximum number of linearly independent rows (or columns) in the matrix. Equivalently, it's the dimension of the column space (the span of the column vectors) or the row space (the span of the row vectors).\n\n**Key Properties:**\n- For an m×n matrix, rank(A) ≤ min(m, n)\n- rank(A) = rank(Aᵀ) (row rank equals column rank)\n- rank(AB) ≤ min(rank(A), rank(B))\n- A square matrix is invertible (has an [inverse]) if and only if its rank equals its size (full rank)\n- If rank(A) < n for an n×n matrix, then det(A) = 0 and A is singular\n- The rank tells you how many \"truly independent\" pieces of information the matrix contains",
      "calculationTitle": "How to Find the Rank of a Matrix Manually",
      "calculation": "The most reliable method for finding rank is using row reduction to [rref]:\n\n**Method: Row Reduction to RREF**\n\n1. Convert the matrix to Reduced Row Echelon Form using Gaussian-Jordan elimination\n2. Count the number of non-zero rows (rows with pivots)\n3. This number is the rank of the matrix\n\n**Why This Works:**\n\n- Row operations don't change the row space (the span of the rows)\n- In RREF, the non-zero rows are clearly linearly independent\n- The number of pivots equals the dimension of both the row space and column space\n\n**Alternative: Column Reduction**\n\nYou can also reduce columns instead of rows, but row reduction is more standard.\n\n**Quick Check Methods:**\n\n- For a square matrix: If you can reduce it to the identity matrix, rank = matrix size (full rank)\n- If a row or column is all zeros, it doesn't contribute to rank\n- If one row/column is a multiple of another, the rank is reduced\n\n**Example:**\n\nFor the matrix [1 2 3; 2 4 6; 0 1 1]:\n- Row 2 is 2×Row 1, so it's linearly dependent\n- Reducing to RREF gives [1 0 1; 0 1 1; 0 0 0]\n- There are 2 non-zero rows, so rank = 2",
      "applicationsTitle": "Applications of Matrix Rank",
      "applications": "Matrix rank appears throughout linear algebra and its applications:\n\n**1. Solving Systems of Equations**: For the system Ax = b:\n- If rank(A) = rank([A|b]) = n (number of variables), there's a unique solution\n- If rank(A) = rank([A|b]) < n, there are infinitely many solutions\n- If rank(A) < rank([A|b]), there's no solution (inconsistent system)\n\nThe rank-nullity theorem: rank(A) + nullity(A) = number of columns\n\n**2. Matrix Invertibility**: A square n×n matrix has an [inverse] if and only if rank(A) = n (full rank). If rank(A) < n, the [determinant] is zero and the matrix is singular.\n\n**3. Linear Independence**: The rank tells you how many vectors in a set are linearly independent. If you have m vectors in n-dimensional space, and rank = m, they're all independent.\n\n**4. Dimension of Subspaces**: \n- Column space dimension = rank(A)\n- Row space dimension = rank(A)\n- Null space dimension = n - rank(A) (nullity)\n\n**5. Data Analysis**: In machine learning, the rank of a data matrix indicates the intrinsic dimensionality of the dataset. Low rank suggests redundancy or that data lies in a lower-dimensional subspace.\n\n**6. Control Theory**: The rank of controllability and observability matrices determines whether a system can be controlled or observed.\n\n**7. Network Analysis**: In graph theory, the rank of incidence matrices relates to connectivity properties of networks.\n\n**8. Compression**: Low-rank matrix approximations are used in image compression, recommender systems, and dimensionality reduction techniques."
    },
    "system-equations": {
      "title": "System of Linear Equations Solver: Complete Guide",
      "introduction": "Solving systems of linear equations is one of the most fundamental problems in linear algebra. Whether you're working with 2 equations and 2 unknowns or larger systems, understanding how to solve them systematically is essential. This guide covers the augmented matrix method using [rref] (Gaussian elimination) to solve any system of linear equations.\n\nOur calculator uses the same method taught in university courses: convert the system to an augmented matrix, apply row operations to reach RREF, and read the solution directly from the result.",
      "definitionTitle": "What is a System of Linear Equations?",
      "definition": "A system of linear equations is a collection of equations of the form:\n\na₁₁x₁ + a₁₂x₂ + ... + a₁ₙxₙ = b₁\na₂₁x₁ + a₂₂x₂ + ... + a₂ₙxₙ = b₂\n...\naₘ₁x₁ + aₘ₂x₂ + ... + aₘₙxₙ = bₘ\n\nIn matrix form, this becomes Ax = B, where:\n- A is the coefficient matrix (m×n)\n- x is the variable vector (n×1)\n- B is the constants vector (m×1)\n\n**Types of Solutions:**\n- **Unique Solution**: Exactly one solution exists (consistent and independent system)\n- **Infinitely Many Solutions**: Free variables exist (consistent but dependent system)\n- **No Solution**: The system is inconsistent (contradictory equations)",
      "calculationTitle": "How to Solve Systems Using Augmented Matrix Method",
      "calculation": "The augmented matrix method is the most systematic approach:\n\n**Step 1: Form the Augmented Matrix**\n\nWrite the system Ax = B as [A | B], where the vertical bar separates coefficients from constants.\n\n**Step 2: Apply Gaussian Elimination**\n\nUse row operations to convert the augmented matrix to [rref] (Reduced Row Echelon Form):\n\n1. **Forward Elimination**: Make all entries below each pivot zero\n2. **Scale Pivots**: Make each pivot equal to 1\n3. **Backward Elimination**: Make all entries above each pivot zero\n\n**Step 3: Read the Solution**\n\nFrom the RREF form:\n- Each pivot column corresponds to a basic variable\n- The rightmost column gives the values\n- If a row is all zeros except the last column (non-zero), the system has no solution\n- If there are fewer pivots than variables, free variables exist (infinite solutions)\n\nThis method is equivalent to using [rref] techniques and works for any system size.",
      "applicationsTitle": "Applications of Solving Linear Systems",
      "applications": "Systems of linear equations appear everywhere in mathematics and science:\n\n**1. Engineering Problems**: Circuit analysis, structural engineering, and optimization problems all reduce to solving linear systems.\n\n**2. Economics**: Input-output models, supply and demand analysis, and economic forecasting use systems of equations.\n\n**3. Computer Graphics**: 3D transformations, ray tracing, and solving for intersection points require solving linear systems.\n\n**4. Data Science**: Linear regression, least squares fitting, and many machine learning algorithms solve systems of equations.\n\n**5. Physics**: Balancing chemical equations, solving for forces in statics, and many physics problems are linear systems.\n\n**6. Game Development**: Collision detection, pathfinding, and physics simulations often require solving linear systems.\n\n**7. Cryptography**: Some encryption methods involve solving systems of linear equations over finite fields.\n\n**8. Network Analysis**: Traffic flow, electrical networks, and social network analysis all use linear systems."
    },
    "cramers-rule": {
      "title": "Cramer's Rule Calculator: Step-by-Step Guide",
      "introduction": "Cramer's Rule is an elegant method for solving systems of linear equations using [determinant]s. While it's computationally expensive for large systems, it's perfect for 2×2 and 3×3 systems and provides a clear, formula-based approach that many students find intuitive.\n\nThis guide explains what Cramer's Rule is, when it can be used, and how to apply it step-by-step. Our calculator shows every [determinant] calculation, making it easy to understand the process.",
      "definitionTitle": "What is Cramer's Rule?",
      "definition": "Cramer's Rule provides an explicit formula for solving the system Ax = B when A is a square, invertible matrix (det(A) ≠ 0).\n\nFor a system of n equations with n unknowns:\n\nx₁ = Dx₁ / D\nx₂ = Dx₂ / D\n...\nxₙ = Dxₙ / D\n\nWhere:\n- D = det(A) is the [determinant] of the coefficient matrix\n- Dxᵢ = det(Aᵢ) is the [determinant] of matrix A with column i replaced by the constants vector B\n\n**When Cramer's Rule Works:**\n- The system must have the same number of equations as unknowns (square coefficient matrix)\n- The coefficient matrix must be invertible (det(A) ≠ 0)\n- Best for 2×2 and 3×3 systems (computationally efficient)\n- For larger systems, [rref] methods are more practical",
      "calculationTitle": "How to Apply Cramer's Rule Step-by-Step",
      "calculation": "**Step 1: Calculate Determinant D**\n\nFind the [determinant] of the coefficient matrix A. If D = 0, Cramer's Rule cannot be applied (the system has no unique solution or is inconsistent).\n\n**Step 2: Calculate Dx, Dy, Dz, etc.**\n\nFor each variable xᵢ:\n1. Replace column i in matrix A with the constants vector B\n2. Calculate the [determinant] of this new matrix\n3. This gives you Dxᵢ\n\n**Step 3: Apply the Formula**\n\nDivide each Dxᵢ by D to get the solution:\n\nxᵢ = Dxᵢ / D\n\n**Example for 2×2 System:**\n\nFor the system:\nax + by = e\ncx + dy = f\n\nD = ad - bc\nDx = ed - bf\nDy = af - ec\n\nx = Dx / D, y = Dy / D\n\n**Advantages:**\n- Direct formula, no row operations needed\n- Clear relationship between [determinant]s and solutions\n- Works well for symbolic calculations\n\n**Disadvantages:**\n- Requires calculating n+1 [determinant]s (computationally expensive for large n)\n- Only works for square, invertible systems\n- For n > 3, [rref] methods are faster",
      "applicationsTitle": "When to Use Cramer's Rule",
      "applications": "Cramer's Rule is particularly useful in these scenarios:\n\n**1. Small Systems (2×2, 3×3)**: When you have 2 or 3 equations, Cramer's Rule is often the fastest method and provides clear formulas.\n\n**2. Symbolic Calculations**: When working with variables instead of numbers, Cramer's Rule gives explicit formulas that are easy to manipulate algebraically.\n\n**3. Teaching and Learning**: Cramer's Rule helps students understand the relationship between [determinant]s and solutions, making it valuable for educational purposes.\n\n**4. Theoretical Analysis**: In proofs and theoretical work, Cramer's Rule provides explicit expressions that are useful for analysis.\n\n**5. Special Cases**: When you need to solve for just one variable in a system, Cramer's Rule lets you calculate only the relevant [determinant]s.\n\n**6. Computer Algebra Systems**: Symbolic math software often uses Cramer's Rule for small systems because it provides exact symbolic solutions.\n\n**When NOT to Use Cramer's Rule:**\n- Systems larger than 3×3 (use [rref] instead)\n- When the coefficient matrix is singular (det = 0)\n- When computational efficiency is critical\n- For systems with more equations than unknowns or vice versa"
    },
    "matrix-power": {
      "title": "Matrix Power Calculator: Computing A^n",
      "introduction": "Computing matrix powers (A², A³, Aⁿ) is essential in many areas of mathematics, from Markov chains to solving systems of differential equations. While the concept is simple—multiply A by itself n times—the calculations can become tedious for large powers or large matrices.\n\nThis guide explains how matrix powers work, when they're useful, and how to compute them efficiently. Our calculator shows each multiplication step, making it easy to understand the process.",
      "definitionTitle": "What is a Matrix Power?",
      "definition": "For a square matrix A, the power Aⁿ (where n is a non-negative integer) is defined as:\n\nAⁿ = A × A × ... × A (n times)\n\n**Special Cases:**\n- A⁰ = I (the identity matrix)\n- A¹ = A\n- A² = A × A\n- A³ = A² × A = A × A × A\n\n**Key Properties:**\n- (Aⁿ)ᵐ = Aⁿᵐ (power of a power)\n- Aⁿ × Aᵐ = Aⁿ⁺ᵐ (multiplying powers)\n- (AB)ⁿ ≠ AⁿBⁿ in general (matrix multiplication is not commutative)\n- If A is invertible, (A⁻¹)ⁿ = (Aⁿ)⁻¹\n- For diagonalizable matrices, powers can be computed more efficiently using eigenvalues",
      "calculationTitle": "How to Calculate Matrix Powers",
      "calculation": "**Method 1: Direct Multiplication (for small powers)**\n\nFor small values of n, simply multiply A by itself repeatedly:\n\n1. Start with A¹ = A\n2. Calculate A² = A × A using [multiplication]\n3. Calculate A³ = A² × A\n4. Continue until you reach Aⁿ\n\n**Method 2: Exponentiation by Squaring (for large powers)**\n\nFor large n, use the binary representation:\n\n- If n is even: Aⁿ = (Aⁿ/²)²\n- If n is odd: Aⁿ = A × (A⁽ⁿ⁻¹⁾/²)²\n\nThis reduces the number of multiplications from n-1 to approximately log₂(n).\n\n**Method 3: Diagonalization (for diagonalizable matrices)**\n\nIf A = PDP⁻¹ where D is diagonal:\n- Aⁿ = PDⁿP⁻¹\n- Dⁿ is simply raising each diagonal element to the power n\n\n**Our Calculator's Approach:**\n\nWe use Method 1 (direct multiplication) and show each step, making it easy to follow the process. For powers greater than 5, we recommend using diagonalization or exponentiation by squaring for efficiency.",
      "applicationsTitle": "Applications of Matrix Powers",
      "applications": "Matrix powers are fundamental in many mathematical and scientific fields:\n\n**1. Markov Chains**: In probability theory, the n-step transition probabilities in a Markov chain are given by Pⁿ, where P is the transition matrix. This tells you the probability of moving from one state to another in n steps.\n\n**2. Systems of Differential Equations**: Solutions to systems like dx/dt = Ax involve matrix exponentials, which are computed using matrix powers in the series expansion: e^A = I + A + A²/2! + A³/3! + ...\n\n**3. Graph Theory**: The (i,j) entry of Aⁿ gives the number of paths of length n from vertex i to vertex j in a graph.\n\n**4. Population Dynamics**: In biology and ecology, matrix powers model population growth over multiple generations.\n\n**5. Computer Graphics**: Repeated transformations (rotations, scaling) are computed using matrix powers.\n\n**6. Cryptography**: Some encryption algorithms use matrix powers over finite fields.\n\n**7. Network Analysis**: Powers of adjacency matrices reveal connectivity and path information in networks.\n\n**8. Control Theory**: In discrete-time systems, the state after n steps is x(n) = Aⁿx(0), where A is the system matrix."
    }
  },
  "FAQ": {
    "inverse": {
      "q1": "What is an inverse matrix?",
      "a1": "An inverse matrix A⁻¹ is a matrix such that A × A⁻¹ = A⁻¹ × A = I, where I is the identity matrix. Not all matrices have inverses—only square matrices with non-zero determinants are invertible.",
      "q2": "How do you find the inverse of a 2×2 matrix?",
      "a2": "For a 2×2 matrix A = [a b; c d], the inverse is A⁻¹ = (1/(ad-bc)) × [d -b; -c a], provided that ad-bc ≠ 0 (the determinant is non-zero).",
      "q3": "What if a matrix has no inverse?",
      "a3": "A matrix has no inverse (is singular) if its determinant is zero. This means the matrix represents a transformation that collapses space, and you cannot reverse it. In such cases, the system of equations Ax = b either has no solution or infinitely many solutions."
    },
    "rref": {
      "q1": "What is reduced row echelon form?",
      "a1": "Reduced row echelon form (RREF) is a special form of a matrix with these properties: 1) All zero rows are at the bottom, 2) The leading entry (pivot) in each non-zero row is 1, 3) Each leading 1 is the only non-zero entry in its column, 4) Each leading 1 appears to the right of the leading 1 in the row above.",
      "q2": "How to convert a matrix to RREF?",
      "a2": "Use Gaussian-Jordan elimination: 1) Start with row echelon form using forward elimination, 2) Scale each pivot row so the leading entry becomes 1, 3) Eliminate all entries above each pivot by adding multiples of the pivot row to rows above it, 4) Continue until all conditions for RREF are satisfied.",
      "q3": "What is the difference between REF and RREF?",
      "a3": "Row Echelon Form (REF) requires only zeros below each pivot, while Reduced Row Echelon Form (RREF) requires zeros both above and below each pivot. RREF also requires that all pivots equal 1. RREF is unique for any matrix, while REF is not.",
      "q4": "Why use RREF for solving systems?",
      "a4": "RREF makes solving systems of linear equations trivial because the solution can be read directly from the final matrix. Each pivot column corresponds to a basic variable, and the rightmost column (in an augmented matrix) gives the values. It also clearly reveals whether a system has no solution, one solution, or infinitely many solutions."
    },
    "multiplication": {
      "q1": "How do you multiply two matrices?",
      "a1": "Matrix multiplication is done by taking the dot product of rows from the first matrix with columns from the second matrix. For matrices A (m×n) and B (n×p), the result C = AB is an m×p matrix where C[i][j] = sum of A[i][k] × B[k][j] for k from 1 to n.",
      "q2": "Can you multiply any two matrices?",
      "a2": "No. For matrix multiplication AB to be defined, the number of columns in A must equal the number of rows in B. The resulting matrix will have the number of rows from A and the number of columns from B.",
      "q3": "Is matrix multiplication commutative?",
      "a3": "No, matrix multiplication is not commutative. In general, AB ≠ BA. However, some special cases exist: if A and B are both diagonal matrices, or if one is the identity matrix, then AB = BA."
    },
    "determinant": {
      "q1": "What is a matrix determinant?",
      "a1": "The determinant is a scalar value that can be computed from a square matrix. It provides important information: if det(A) = 0, the matrix is singular (not invertible). If det(A) ≠ 0, the matrix is invertible. The determinant also represents the scaling factor of the linear transformation represented by the matrix.",
      "q2": "How do you calculate the determinant of a 2×2 matrix?",
      "a2": "For a 2×2 matrix [a b; c d], the determinant is ad - bc. This is the simplest case and serves as the base for calculating larger determinants using cofactor expansion.",
      "q3": "What is cofactor expansion?",
      "a3": "Cofactor expansion (Laplace expansion) is a method to calculate determinants by expanding along a row or column. For each element aᵢⱼ, you calculate its cofactor Cᵢⱼ = (-1)ⁱ⁺ʲ × Mᵢⱼ, where Mᵢⱼ is the minor (determinant of the submatrix). Then det(A) = sum of aᵢⱼ × Cᵢⱼ for any row i or column j."
    },
    "eigenvalue": {
      "q1": "What are eigenvalues and eigenvectors?",
      "a1": "An eigenvector v of a matrix A is a non-zero vector such that Av = λv, where λ is the eigenvalue. Eigenvectors represent directions that remain unchanged (except for scaling) under the linear transformation, and eigenvalues represent the scaling factors.",
      "q2": "How do you find eigenvalues?",
      "a2": "Eigenvalues are found by solving the characteristic equation det(A - λI) = 0. This gives a polynomial in λ called the characteristic polynomial. The roots of this polynomial are the eigenvalues.",
      "q3": "What are eigenvalues used for?",
      "a3": "Eigenvalues and eigenvectors are used in principal component analysis (PCA), Google's PageRank algorithm, quantum mechanics, vibration analysis, stability analysis, and many other applications in mathematics, physics, and computer science."
    },
    "rank": {
      "q1": "What is the rank of a matrix?",
      "a1": "The rank of a matrix is the maximum number of linearly independent rows (or columns). It equals the dimension of the column space and row space. For an m×n matrix, rank ≤ min(m, n).",
      "q2": "How do you find the rank?",
      "a2": "The most reliable method is to convert the matrix to reduced row echelon form (RREF) and count the number of non-zero rows (pivot rows). This number is the rank.",
      "q3": "What does rank tell you about a system of equations?",
      "a3": "For the system Ax = b: if rank(A) = rank([A|b]) = n (number of variables), there's a unique solution. If rank(A) = rank([A|b]) < n, there are infinitely many solutions. If rank(A) < rank([A|b]), there's no solution."
    },
    "system-equations": {
      "q1": "How do you solve a system of linear equations?",
      "a1": "The most systematic method is to convert the system to an augmented matrix [A|B] and use Gaussian elimination (row operations) to reduce it to RREF. The solution can then be read directly from the RREF matrix.",
      "q2": "When does a system have no solution?",
      "a2": "A system has no solution (is inconsistent) when, after reducing to RREF, you get a row of the form [0 0 ... 0 | c] where c ≠ 0. This represents an impossible equation like 0 = c.",
      "q3": "When does a system have infinitely many solutions?",
      "a3": "A system has infinitely many solutions when there are free variables—variables that don't correspond to pivot columns. In RREF, if the number of pivots is less than the number of variables, free variables exist and the system has infinitely many solutions."
    },
    "cramers-rule": {
      "q1": "What is Cramer's Rule?",
      "a1": "Cramer's Rule is a method for solving systems of linear equations using determinants. For a system Ax = B, each variable xᵢ = Dxᵢ/D, where D is the determinant of the coefficient matrix and Dxᵢ is the determinant with column i replaced by the constants vector.",
      "q2": "When can you use Cramer's Rule?",
      "a2": "Cramer's Rule only works for square systems (same number of equations as unknowns) where the coefficient matrix is invertible (determinant ≠ 0). It's most practical for 2×2 and 3×3 systems.",
      "q3": "Why is Cramer's Rule not used for large systems?",
      "a3": "Cramer's Rule requires calculating n+1 determinants for an n×n system, which is computationally expensive (O(n!) for each determinant). For systems larger than 3×3, Gaussian elimination (RREF) is much faster."
    },
    "matrix-power": {
      "q1": "How do you calculate A^n for a matrix?",
      "a1": "For small powers, multiply A by itself n times: A² = A × A, A³ = A² × A, etc. For large powers, use exponentiation by squaring or diagonalization for efficiency.",
      "q2": "What is A^0 for any matrix?",
      "a2": "A^0 = I (the identity matrix) for any square matrix A, just like any number raised to the power of 0 equals 1.",
      "q3": "Can you compute negative powers?",
      "a3": "Yes, if A is invertible, A⁻ⁿ = (A⁻¹)ⁿ. This requires first finding the inverse matrix, then raising it to the power n."
    }
  }
}